{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感知机\n",
    "感知机是根据输入实例的特征向量 x 对其进行二类分类的线性模型：\n",
    "$$ f(x)=sign(w\\cdot x+b) $$\n",
    "感知机模型对应于输入空间（特征空间）中的分离超平面 $ w\\cdot x+b=0 $.其中w是超平面的法向量，b是超平面的截距。\n",
    "\n",
    "可见感知机是一种线性分类模型，属于判别模型。\n",
    "***\n",
    "## 感知机学习的假设\n",
    "感知机学习的重要前提假设是训练数据集是线性可分的。\n",
    "***\n",
    "## 感知机学习策略\n",
    "感知机学的策略是极小化损失函数。\n",
    "\n",
    "损失函数的一个自然选择是误分类点的总数。但是，这样的损失函数不是参数 w, b的连续可导的函数，不易于优化。所以通常是选择误分类点到超平面 S 的总距离：\n",
    "$$ L(w,b)=-\\sum_{x_i\\in M}y_i(w\\cdot x_i+b) $$\n",
    "学习的策略就是求得使 L(w,b) 为最小值的 w 和 b。其中 M 是误分类点的集合。\n",
    "***\n",
    "## 感知机学习的算法\n",
    "感知机学习算法是基于随机梯度下降法的对损失函数的最优化算法，有原始形式和对偶形式，算法简单易于实现。\n",
    "### 原始形式\n",
    "$$ \\min_{w,b}L(w,b)=-\\sum_{x_i\\in M}y_i(w\\cdot x_i+b) $$\n",
    "首先，任意选取一个超平面$ w_0, b_0 $,然后用梯度下降法不断地极小化目标函数。极小化的过程中不是一次使 M 中所有误分类点得梯度下降，而是一次随机选取一个误分类点，使其梯度下降。\n",
    "$$ \\nabla_wL(w,b)=-\\sum_{x_i\\in M}y_ix_i $$\n",
    "$$ \\nabla_bL(w,b)=-\\sum_{x_i\\in M}y_i $$\n",
    "\n",
    "随机选取一个误分类点$ (x_i,y_i) $，对 w,b 进行更新：\n",
    "$$ w\\leftarrow w+\\eta y_ix_i $$\n",
    "$$ b\\leftarrow b+\\eta y_i $$\n",
    "其中$ \\eta(0<\\eta\\leq1) $是学习率。\n",
    "\n",
    "### 对偶形式\n",
    "对偶形式的基本想法是，将 w 和 b 表示为是咧 $ x_i $ 和标记 $ y_i $的线性组合的形式，通过求解其系数而得到 w 和 b。\n",
    "$$ w\\leftarrow w+\\eta y_ix_i $$\n",
    "$$ b\\leftarrow b+\\eta y_i $$\n",
    "逐步修改 w,b，设修改 n 次，则 w,b 关于$ (x_i,y_i) $ 的增量分别是 $ \\alpha_iy_ix_i $ 和 $ \\alpha_iy_i $, 这里 $ \\alpha_i=n_i\\eta $。最后学习到的 w,b 可以分别表示为：\n",
    "$$ w=\\sum_{i=1}^{N}\\alpha_iy_ix_i $$\n",
    "$$ b=\\sum_{i=1}^{N}\\alpha_iy_i $$\n",
    "这里， $ \\alpha_i\\geq0, i=1,2,...,N $,当 $ \\eta=1 $时，表示第i个是实例点由于误分类而进行更新的次数，实例点更新次数越多，说明它距离分离超平面越近，也就越难区分，该点对学习结果的影响最大。\n",
    "\n",
    "感知机模型对偶形式： $$f(x)=sign(\\sum_{j=1}^{N}\\alpha_jy_jx_j\\cdot x+b) $$ 其中$$\\alpha=(\\alpha_1,\\alpha_2,...,\\alpha_N)^T$$\n",
    "学习时初始化 $ \\alpha \\leftarrow 0, b \\leftarrow 0 $, 在训练集中找分类错误的点，即：\n",
    " $$ y_i(\\sum_{j=1}^{N}\\alpha_jy_jx_j\\cdot x_i+b)\\leq 0 $$\n",
    " 然后更新：\n",
    " $$ \\alpha_i \\leftarrow \\alpha_i+\\eta$$\n",
    "$$ b\\leftarrow b+\\eta y_i $$\n",
    "知道训练集中所有点正确分类\n",
    "\n",
    "对偶形式中训练实例仅以内积的形式出现，为了方便，可以预先将训练集中实例间的内积计算出来以矩阵的形式存储，即 Gram 矩阵。\n",
    "***\n",
    "## 总结\n",
    "\n",
    "* 当训练数据集线性可分的时候，感知机学习算法是收敛的，感知机算法在训练数据集上的误分类次数 k 满足不等式:\n",
    "$$ k\\leq (\\frac{R}{\\gamma})^2  $$\n",
    "具体证明可见 <font color=blue>李航《统计学习方法》或 林轩田《机器学习基石》</font>。\n",
    "\n",
    "* 当训练当训练数据集线性可分的时候，感知机学习算法存在无穷多个解，其解由于不同的初值或不同的迭代顺序而可能不同，即存在多个分离超平面能把数据集分开。\n",
    "\n",
    "* 感知机学习算法简单易求解，但一般的感知机算法不能解决异或等线性不可分的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 比较两个模型\n",
    "分别从原始模型和对偶模型中获取参数，可以看出，这两个模型的分离超平面都不同，但是都能正确进行分类，这验证了总结中的结论。\n",
    "\n",
    "当训练当训练数据集线性可分的时候，感知机学习算法存在无穷多个解，其解由于不同的初值或不同的迭代顺序而可能不同，即存在多个分离超平面能把数据集分开。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
